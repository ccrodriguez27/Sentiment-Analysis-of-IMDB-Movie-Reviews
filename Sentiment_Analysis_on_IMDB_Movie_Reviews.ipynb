{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMYbPn6d0JjIqWpedSi86Zj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ccrodriguez27/Sentiment-Analysis-of-IMDB-Movie-Reviews/blob/main/Sentiment_Analysis_on_IMDB_Movie_Reviews.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1 - Import Libraries"
      ],
      "metadata": {
        "id": "mVrD5OApJSOE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OQU9gGlbIv0p",
        "outputId": "c54134f5-f03c-46da-fcba-d08797d301b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "import shutil\n",
        "\n",
        "import string\n",
        "import re\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.tokenize import WordPunctTokenizer\n",
        "\n",
        "import random as rnd\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = '/content/drive/MyDrive/'\n",
        "data = pd.read_csv(data_dir+'IMDB Dataset.csv')"
      ],
      "metadata": {
        "id": "vhl62XKCMGAG"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2 - Importing the Data"
      ],
      "metadata": {
        "id": "trARrl6oJZoi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "id": "xnbUqyO9I4iA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "780544ce-b78a-42c9-925f-a9e36ef6edc0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                              review sentiment\n",
              "0  One of the other reviewers has mentioned that ...  positive\n",
              "1  A wonderful little production. <br /><br />The...  positive\n",
              "2  I thought this was a wonderful way to spend ti...  positive\n",
              "3  Basically there's a family where a little boy ...  negative\n",
              "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-5c83e235-b14c-4f2e-ac24-57525d0bf13d\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>One of the other reviewers has mentioned that ...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I thought this was a wonderful way to spend ti...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Basically there's a family where a little boy ...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "      \n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5c83e235-b14c-4f2e-ac24-57525d0bf13d')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "      \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-5c83e235-b14c-4f2e-ac24-57525d0bf13d button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-5c83e235-b14c-4f2e-ac24-57525d0bf13d');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "  \n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.groupby(by = 'sentiment').count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "id": "4e6JvmBTNdqA",
        "outputId": "22dc4c6c-f6ab-4669-8f95-d6995a4ee6df"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           review\n",
              "sentiment        \n",
              "negative    25000\n",
              "positive    25000"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-020406ef-596d-4cf2-871a-b4b630dda8b1\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sentiment</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>negative</th>\n",
              "      <td>25000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>positive</th>\n",
              "      <td>25000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "      \n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-020406ef-596d-4cf2-871a-b4b630dda8b1')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "      \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-020406ef-596d-4cf2-871a-b4b630dda8b1 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-020406ef-596d-4cf2-871a-b4b630dda8b1');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "  \n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = data.drop_duplicates()"
      ],
      "metadata": {
        "id": "lYhcSRaFZMhm"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.groupby(by = 'sentiment').count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "id": "_8kEyZFcZSI0",
        "outputId": "250126b9-589e-4dc1-c176-03253a18c3aa"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           review\n",
              "sentiment        \n",
              "negative    24698\n",
              "positive    24884"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-66dc7f32-fc5b-498c-ad8b-f4b0e0d785ed\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sentiment</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>negative</th>\n",
              "      <td>24698</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>positive</th>\n",
              "      <td>24884</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "      \n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-66dc7f32-fc5b-498c-ad8b-f4b0e0d785ed')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "      \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-66dc7f32-fc5b-498c-ad8b-f4b0e0d785ed button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-66dc7f32-fc5b-498c-ad8b-f4b0e0d785ed');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "  \n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_val_test_split(data, train_ratio=0.7, val_ratio=0.15):\n",
        "    # Separate positive and negative samples\n",
        "    pos_data = data[data['sentiment'] == 'positive']\n",
        "    neg_data = data[data['sentiment'] == 'negative']\n",
        "\n",
        "    # Calculate the number of samples for each split\n",
        "    pos_count = len(pos_data)\n",
        "    neg_count = len(neg_data)\n",
        "\n",
        "    train_pos_count = int(pos_count * train_ratio)\n",
        "    val_pos_count = int(pos_count * val_ratio)\n",
        "    test_pos_count = pos_count - train_pos_count - val_pos_count\n",
        "\n",
        "    train_neg_count = int(neg_count * train_ratio)\n",
        "    val_neg_count = int(neg_count * val_ratio)\n",
        "    test_neg_count = neg_count - train_neg_count - val_neg_count\n",
        "\n",
        "    # Split the data\n",
        "    train_pos = pos_data[:train_pos_count]['review'].reset_index(drop=True)\n",
        "    val_pos = pos_data[train_pos_count:train_pos_count + val_pos_count]['review'].reset_index(drop=True)\n",
        "    test_pos = pos_data[train_pos_count + val_pos_count:]['review'].reset_index(drop=True)\n",
        "\n",
        "    train_neg = neg_data[:train_neg_count]['review'].reset_index(drop=True)\n",
        "    val_neg = neg_data[train_neg_count:train_neg_count + val_neg_count]['review'].reset_index(drop=True)\n",
        "    test_neg = neg_data[train_neg_count + val_neg_count:]['review'].reset_index(drop=True)\n",
        "\n",
        "    # Combine data\n",
        "    train_x = train_pos.tolist() + train_neg.tolist()\n",
        "    val_x = val_pos.tolist() + val_neg.tolist()\n",
        "    test_x = test_pos.tolist() + test_neg.tolist()\n",
        "\n",
        "    # Set labels\n",
        "    train_y = [1] * len(train_pos) + [0] * len(train_neg)\n",
        "    val_y = [1] * len(val_pos) + [0] * len(val_neg)\n",
        "    test_y = [1] * len(test_pos) + [0] * len(test_neg)\n",
        "\n",
        "    return train_pos, train_neg, train_x, train_y, val_pos, val_neg, val_x, val_y, test_pos, test_neg, test_x, test_y"
      ],
      "metadata": {
        "id": "LTT47TgYZV_L"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_pos, train_neg, train_x, train_y, val_pos, val_neg, val_x, val_y, test_pos, test_neg, test_x, test_y = train_val_test_split(data, train_ratio=0.7, val_ratio=0.15)\n",
        "\n",
        "print(f\"length of train_x {len(train_x)}\")\n",
        "print(f\"length of val_x {len(val_x)}\")\n",
        "print(f\"length of test_x {len(test_x)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hOvuCCiLZW96",
        "outputId": "d74438cf-16f0-48cb-d2dc-6e23ee09b8c7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of train_x 34706\n",
            "length of val_x 7436\n",
            "length of test_x 7440\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords_english = stopwords.words('english')\n",
        "stemmer = PorterStemmer()\n",
        "tokenizer = WordPunctTokenizer()"
      ],
      "metadata": {
        "id": "3biEMcOiZZo6"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_review(review):\n",
        "    '''\n",
        "    Input:\n",
        "        review: a string containing a review\n",
        "    Output:\n",
        "        review_clean: a list of words containing the processed review\n",
        "\n",
        "    '''\n",
        "    # Remove HTML tags\n",
        "    review = re.sub(r'<.*?>', ' ', review)\n",
        "    review = re.sub(r'[^A-Za-z0-9\\s]', '', review)\n",
        "\n",
        "    review_tokens = tokenizer.tokenize(review.lower())\n",
        "\n",
        "    reviews_clean = []\n",
        "    for word in review_tokens:\n",
        "        if (word not in stopwords_english and # remove stopwords\n",
        "            word not in string.punctuation): # remove punctuation\n",
        "\n",
        "            stem_word = stemmer.stem(word) # stemming word\n",
        "            reviews_clean.append(stem_word)\n",
        "\n",
        "    return reviews_clean"
      ],
      "metadata": {
        "id": "vR38-Tv6ZjOy"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Try out function that processes reviews\n",
        "print(\"original review at training position 0\")\n",
        "print(train_pos[1])\n",
        "\n",
        "print(\"Review at training position 0 after processing:\")\n",
        "process_review(train_pos[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZuehguUQZk7m",
        "outputId": "c45562a4-533e-481b-a566-875ac99cb6a0"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "original review at training position 0\n",
            "A wonderful little production. <br /><br />The filming technique is very unassuming- very old-time-BBC fashion and gives a comforting, and sometimes discomforting, sense of realism to the entire piece. <br /><br />The actors are extremely well chosen- Michael Sheen not only \"has got all the polari\" but he has all the voices down pat too! You can truly see the seamless editing guided by the references to Williams' diary entries, not only is it well worth the watching but it is a terrificly written and performed piece. A masterful production about one of the great master's of comedy and his life. <br /><br />The realism really comes home with the little things: the fantasy of the guard which, rather than use the traditional 'dream' techniques remains solid then disappears. It plays on our knowledge and our senses, particularly with the scenes concerning Orton and Halliwell and the sets (particularly of their flat with Halliwell's murals decorating every surface) are terribly well done.\n",
            "Review at training position 0 after processing:\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['wonder',\n",
              " 'littl',\n",
              " 'product',\n",
              " 'film',\n",
              " 'techniqu',\n",
              " 'unassum',\n",
              " 'oldtimebbc',\n",
              " 'fashion',\n",
              " 'give',\n",
              " 'comfort',\n",
              " 'sometim',\n",
              " 'discomfort',\n",
              " 'sens',\n",
              " 'realism',\n",
              " 'entir',\n",
              " 'piec',\n",
              " 'actor',\n",
              " 'extrem',\n",
              " 'well',\n",
              " 'chosen',\n",
              " 'michael',\n",
              " 'sheen',\n",
              " 'got',\n",
              " 'polari',\n",
              " 'voic',\n",
              " 'pat',\n",
              " 'truli',\n",
              " 'see',\n",
              " 'seamless',\n",
              " 'edit',\n",
              " 'guid',\n",
              " 'refer',\n",
              " 'william',\n",
              " 'diari',\n",
              " 'entri',\n",
              " 'well',\n",
              " 'worth',\n",
              " 'watch',\n",
              " 'terrificli',\n",
              " 'written',\n",
              " 'perform',\n",
              " 'piec',\n",
              " 'master',\n",
              " 'product',\n",
              " 'one',\n",
              " 'great',\n",
              " 'master',\n",
              " 'comedi',\n",
              " 'life',\n",
              " 'realism',\n",
              " 'realli',\n",
              " 'come',\n",
              " 'home',\n",
              " 'littl',\n",
              " 'thing',\n",
              " 'fantasi',\n",
              " 'guard',\n",
              " 'rather',\n",
              " 'use',\n",
              " 'tradit',\n",
              " 'dream',\n",
              " 'techniqu',\n",
              " 'remain',\n",
              " 'solid',\n",
              " 'disappear',\n",
              " 'play',\n",
              " 'knowledg',\n",
              " 'sens',\n",
              " 'particularli',\n",
              " 'scene',\n",
              " 'concern',\n",
              " 'orton',\n",
              " 'halliwel',\n",
              " 'set',\n",
              " 'particularli',\n",
              " 'flat',\n",
              " 'halliwel',\n",
              " 'mural',\n",
              " 'decor',\n",
              " 'everi',\n",
              " 'surfac',\n",
              " 'terribl',\n",
              " 'well',\n",
              " 'done']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2 Building the Vocabulary based on the training data\n",
        "This is done by:\n",
        "\n",
        "- Assigning an index to every word by iterating over the training set.\n",
        "- Including special tokens, namely:\n",
        "  1. **PAD**: padding\n",
        "  2. <**/e**>: end of line\n",
        "  3. **UNK**: a token representing any word that is not in the vocabulary.\n"
      ],
      "metadata": {
        "id": "EeTY4s9jJxIT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the vocabulary\n",
        "def get_vocab(train_x):\n",
        "\n",
        "    # Include special tokens\n",
        "    # started with pad, end of line and unk tokens\n",
        "    Vocab = {'__PAD__': 0, '__</e>__': 1, '__UNK__': 2}\n",
        "\n",
        "    # Note that we build vocab using training data\n",
        "    for review in train_x:\n",
        "        processed_review = process_review(review)\n",
        "        for word in processed_review:\n",
        "            if word not in Vocab:\n",
        "                Vocab[word] = len(Vocab)\n",
        "\n",
        "    return Vocab"
      ],
      "metadata": {
        "id": "eUB7PI4NZng_"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Vocab = get_vocab(train_x)\n",
        "\n",
        "print(\"Total words in vocab are\",len(Vocab))\n",
        "display(Vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "z6VojJEgZpLV",
        "outputId": "4f6981f9-6d5a-430e-95e4-31b329f375ea"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total words in vocab are 102924\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "{'__PAD__': 0,\n",
              " '__</e>__': 1,\n",
              " '__UNK__': 2,\n",
              " 'one': 3,\n",
              " 'review': 4,\n",
              " 'mention': 5,\n",
              " 'watch': 6,\n",
              " '1': 7,\n",
              " 'oz': 8,\n",
              " 'episod': 9,\n",
              " 'youll': 10,\n",
              " 'hook': 11,\n",
              " 'right': 12,\n",
              " 'exactli': 13,\n",
              " 'happen': 14,\n",
              " 'first': 15,\n",
              " 'thing': 16,\n",
              " 'struck': 17,\n",
              " 'brutal': 18,\n",
              " 'unflinch': 19,\n",
              " 'scene': 20,\n",
              " 'violenc': 21,\n",
              " 'set': 22,\n",
              " 'word': 23,\n",
              " 'go': 24,\n",
              " 'trust': 25,\n",
              " 'show': 26,\n",
              " 'faint': 27,\n",
              " 'heart': 28,\n",
              " 'timid': 29,\n",
              " 'pull': 30,\n",
              " 'punch': 31,\n",
              " 'regard': 32,\n",
              " 'drug': 33,\n",
              " 'sex': 34,\n",
              " 'hardcor': 35,\n",
              " 'classic': 36,\n",
              " 'use': 37,\n",
              " 'call': 38,\n",
              " 'nicknam': 39,\n",
              " 'given': 40,\n",
              " 'oswald': 41,\n",
              " 'maximum': 42,\n",
              " 'secur': 43,\n",
              " 'state': 44,\n",
              " 'penitentari': 45,\n",
              " 'focus': 46,\n",
              " 'mainli': 47,\n",
              " 'emerald': 48,\n",
              " 'citi': 49,\n",
              " 'experiment': 50,\n",
              " 'section': 51,\n",
              " 'prison': 52,\n",
              " 'cell': 53,\n",
              " 'glass': 54,\n",
              " 'front': 55,\n",
              " 'face': 56,\n",
              " 'inward': 57,\n",
              " 'privaci': 58,\n",
              " 'high': 59,\n",
              " 'agenda': 60,\n",
              " 'em': 61,\n",
              " 'home': 62,\n",
              " 'manyaryan': 63,\n",
              " 'muslim': 64,\n",
              " 'gangsta': 65,\n",
              " 'latino': 66,\n",
              " 'christian': 67,\n",
              " 'italian': 68,\n",
              " 'irish': 69,\n",
              " 'moreso': 70,\n",
              " 'scuffl': 71,\n",
              " 'death': 72,\n",
              " 'stare': 73,\n",
              " 'dodgi': 74,\n",
              " 'deal': 75,\n",
              " 'shadi': 76,\n",
              " 'agreement': 77,\n",
              " 'never': 78,\n",
              " 'far': 79,\n",
              " 'away': 80,\n",
              " 'would': 81,\n",
              " 'say': 82,\n",
              " 'main': 83,\n",
              " 'appeal': 84,\n",
              " 'due': 85,\n",
              " 'fact': 86,\n",
              " 'goe': 87,\n",
              " 'wouldnt': 88,\n",
              " 'dare': 89,\n",
              " 'forget': 90,\n",
              " 'pretti': 91,\n",
              " 'pictur': 92,\n",
              " 'paint': 93,\n",
              " 'mainstream': 94,\n",
              " 'audienc': 95,\n",
              " 'charm': 96,\n",
              " 'romanceoz': 97,\n",
              " 'doesnt': 98,\n",
              " 'mess': 99,\n",
              " 'around': 100,\n",
              " 'ever': 101,\n",
              " 'saw': 102,\n",
              " 'nasti': 103,\n",
              " 'surreal': 104,\n",
              " 'couldnt': 105,\n",
              " 'readi': 106,\n",
              " 'develop': 107,\n",
              " 'tast': 108,\n",
              " 'got': 109,\n",
              " 'accustom': 110,\n",
              " 'level': 111,\n",
              " 'graphic': 112,\n",
              " 'injustic': 113,\n",
              " 'crook': 114,\n",
              " 'guard': 115,\n",
              " 'wholl': 116,\n",
              " 'sold': 117,\n",
              " 'nickel': 118,\n",
              " 'inmat': 119,\n",
              " 'kill': 120,\n",
              " 'order': 121,\n",
              " 'get': 122,\n",
              " 'well': 123,\n",
              " 'manner': 124,\n",
              " 'middl': 125,\n",
              " 'class': 126,\n",
              " 'turn': 127,\n",
              " 'bitch': 128,\n",
              " 'lack': 129,\n",
              " 'street': 130,\n",
              " 'skill': 131,\n",
              " 'experi': 132,\n",
              " 'may': 133,\n",
              " 'becom': 134,\n",
              " 'comfort': 135,\n",
              " 'uncomfort': 136,\n",
              " 'viewingthat': 137,\n",
              " 'touch': 138,\n",
              " 'darker': 139,\n",
              " 'side': 140,\n",
              " 'wonder': 141,\n",
              " 'littl': 142,\n",
              " 'product': 143,\n",
              " 'film': 144,\n",
              " 'techniqu': 145,\n",
              " 'unassum': 146,\n",
              " 'oldtimebbc': 147,\n",
              " 'fashion': 148,\n",
              " 'give': 149,\n",
              " 'sometim': 150,\n",
              " 'discomfort': 151,\n",
              " 'sens': 152,\n",
              " 'realism': 153,\n",
              " 'entir': 154,\n",
              " 'piec': 155,\n",
              " 'actor': 156,\n",
              " 'extrem': 157,\n",
              " 'chosen': 158,\n",
              " 'michael': 159,\n",
              " 'sheen': 160,\n",
              " 'polari': 161,\n",
              " 'voic': 162,\n",
              " 'pat': 163,\n",
              " 'truli': 164,\n",
              " 'see': 165,\n",
              " 'seamless': 166,\n",
              " 'edit': 167,\n",
              " 'guid': 168,\n",
              " 'refer': 169,\n",
              " 'william': 170,\n",
              " 'diari': 171,\n",
              " 'entri': 172,\n",
              " 'worth': 173,\n",
              " 'terrificli': 174,\n",
              " 'written': 175,\n",
              " 'perform': 176,\n",
              " 'master': 177,\n",
              " 'great': 178,\n",
              " 'comedi': 179,\n",
              " 'life': 180,\n",
              " 'realli': 181,\n",
              " 'come': 182,\n",
              " 'fantasi': 183,\n",
              " 'rather': 184,\n",
              " 'tradit': 185,\n",
              " 'dream': 186,\n",
              " 'remain': 187,\n",
              " 'solid': 188,\n",
              " 'disappear': 189,\n",
              " 'play': 190,\n",
              " 'knowledg': 191,\n",
              " 'particularli': 192,\n",
              " 'concern': 193,\n",
              " 'orton': 194,\n",
              " 'halliwel': 195,\n",
              " 'flat': 196,\n",
              " 'mural': 197,\n",
              " 'decor': 198,\n",
              " 'everi': 199,\n",
              " 'surfac': 200,\n",
              " 'terribl': 201,\n",
              " 'done': 202,\n",
              " 'thought': 203,\n",
              " 'way': 204,\n",
              " 'spend': 205,\n",
              " 'time': 206,\n",
              " 'hot': 207,\n",
              " 'summer': 208,\n",
              " 'weekend': 209,\n",
              " 'sit': 210,\n",
              " 'air': 211,\n",
              " 'condit': 212,\n",
              " 'theater': 213,\n",
              " 'lightheart': 214,\n",
              " 'plot': 215,\n",
              " 'simplist': 216,\n",
              " 'dialogu': 217,\n",
              " 'witti': 218,\n",
              " 'charact': 219,\n",
              " 'likabl': 220,\n",
              " 'even': 221,\n",
              " 'bread': 222,\n",
              " 'suspect': 223,\n",
              " 'serial': 224,\n",
              " 'killer': 225,\n",
              " 'disappoint': 226,\n",
              " 'realiz': 227,\n",
              " 'match': 228,\n",
              " 'point': 229,\n",
              " '2': 230,\n",
              " 'risk': 231,\n",
              " 'addict': 232,\n",
              " 'proof': 233,\n",
              " 'woodi': 234,\n",
              " 'allen': 235,\n",
              " 'still': 236,\n",
              " 'fulli': 237,\n",
              " 'control': 238,\n",
              " 'style': 239,\n",
              " 'mani': 240,\n",
              " 'us': 241,\n",
              " 'grown': 242,\n",
              " 'love': 243,\n",
              " 'id': 244,\n",
              " 'laugh': 245,\n",
              " 'year': 246,\n",
              " 'decad': 247,\n",
              " 'ive': 248,\n",
              " 'impress': 249,\n",
              " 'scarlet': 250,\n",
              " 'johanson': 251,\n",
              " 'manag': 252,\n",
              " 'tone': 253,\n",
              " 'sexi': 254,\n",
              " 'imag': 255,\n",
              " 'jump': 256,\n",
              " 'averag': 257,\n",
              " 'spirit': 258,\n",
              " 'young': 259,\n",
              " 'woman': 260,\n",
              " 'crown': 261,\n",
              " 'jewel': 262,\n",
              " 'career': 263,\n",
              " 'wittier': 264,\n",
              " 'devil': 265,\n",
              " 'wear': 266,\n",
              " 'prada': 267,\n",
              " 'interest': 268,\n",
              " 'superman': 269,\n",
              " 'friend': 270,\n",
              " 'petter': 271,\n",
              " 'mattei': 272,\n",
              " 'money': 273,\n",
              " 'visual': 274,\n",
              " 'stun': 275,\n",
              " 'mr': 276,\n",
              " 'offer': 277,\n",
              " 'vivid': 278,\n",
              " 'portrait': 279,\n",
              " 'human': 280,\n",
              " 'relat': 281,\n",
              " 'movi': 282,\n",
              " 'seem': 283,\n",
              " 'tell': 284,\n",
              " 'power': 285,\n",
              " 'success': 286,\n",
              " 'peopl': 287,\n",
              " 'differ': 288,\n",
              " 'situat': 289,\n",
              " 'encount': 290,\n",
              " 'variat': 291,\n",
              " 'arthur': 292,\n",
              " 'schnitzler': 293,\n",
              " 'theme': 294,\n",
              " 'director': 295,\n",
              " 'transfer': 296,\n",
              " 'action': 297,\n",
              " 'present': 298,\n",
              " 'new': 299,\n",
              " 'york': 300,\n",
              " 'meet': 301,\n",
              " 'connect': 302,\n",
              " 'anoth': 303,\n",
              " 'next': 304,\n",
              " 'person': 305,\n",
              " 'know': 306,\n",
              " 'previou': 307,\n",
              " 'contact': 308,\n",
              " 'stylishli': 309,\n",
              " 'sophist': 310,\n",
              " 'luxuri': 311,\n",
              " 'look': 312,\n",
              " 'taken': 313,\n",
              " 'live': 314,\n",
              " 'world': 315,\n",
              " 'habitat': 316,\n",
              " 'soul': 317,\n",
              " 'stage': 318,\n",
              " 'loneli': 319,\n",
              " 'inhabit': 320,\n",
              " 'big': 321,\n",
              " 'best': 322,\n",
              " 'place': 323,\n",
              " 'find': 324,\n",
              " 'sincer': 325,\n",
              " 'fulfil': 326,\n",
              " 'discern': 327,\n",
              " 'case': 328,\n",
              " 'act': 329,\n",
              " 'good': 330,\n",
              " 'direct': 331,\n",
              " 'steve': 332,\n",
              " 'buscemi': 333,\n",
              " 'rosario': 334,\n",
              " 'dawson': 335,\n",
              " 'carol': 336,\n",
              " 'kane': 337,\n",
              " 'imperioli': 338,\n",
              " 'adrian': 339,\n",
              " 'grenier': 340,\n",
              " 'rest': 341,\n",
              " 'talent': 342,\n",
              " 'cast': 343,\n",
              " 'make': 344,\n",
              " 'aliv': 345,\n",
              " 'wish': 346,\n",
              " 'luck': 347,\n",
              " 'await': 348,\n",
              " 'anxious': 349,\n",
              " 'work': 350,\n",
              " 'probabl': 351,\n",
              " 'alltim': 352,\n",
              " 'favorit': 353,\n",
              " 'stori': 354,\n",
              " 'selfless': 355,\n",
              " 'sacrific': 356,\n",
              " 'dedic': 357,\n",
              " 'nobl': 358,\n",
              " 'caus': 359,\n",
              " 'preachi': 360,\n",
              " 'bore': 361,\n",
              " 'old': 362,\n",
              " 'despit': 363,\n",
              " 'seen': 364,\n",
              " '15': 365,\n",
              " 'last': 366,\n",
              " '25': 367,\n",
              " 'paul': 368,\n",
              " 'luka': 369,\n",
              " 'bring': 370,\n",
              " 'tear': 371,\n",
              " 'eye': 372,\n",
              " 'bett': 373,\n",
              " 'davi': 374,\n",
              " 'sympathet': 375,\n",
              " 'role': 376,\n",
              " 'delight': 377,\n",
              " 'kid': 378,\n",
              " 'grandma': 379,\n",
              " 'like': 380,\n",
              " 'dressedup': 381,\n",
              " 'midget': 382,\n",
              " 'children': 383,\n",
              " 'fun': 384,\n",
              " 'mother': 385,\n",
              " 'slow': 386,\n",
              " 'awaken': 387,\n",
              " 'what': 388,\n",
              " 'roof': 389,\n",
              " 'believ': 390,\n",
              " 'startl': 391,\n",
              " 'dozen': 392,\n",
              " 'thumb': 393,\n",
              " 'theyd': 394,\n",
              " 'sure': 395,\n",
              " 'resurrect': 396,\n",
              " 'date': 397,\n",
              " 'seahunt': 398,\n",
              " 'seri': 399,\n",
              " 'tech': 400,\n",
              " 'today': 401,\n",
              " 'back': 402,\n",
              " 'excit': 403,\n",
              " 'mei': 404,\n",
              " 'grew': 405,\n",
              " 'black': 406,\n",
              " 'white': 407,\n",
              " 'tv': 408,\n",
              " 'gunsmok': 409,\n",
              " 'hero': 410,\n",
              " 'weekyou': 411,\n",
              " 'vote': 412,\n",
              " 'comeback': 413,\n",
              " 'sea': 414,\n",
              " 'huntw': 415,\n",
              " 'need': 416,\n",
              " 'chang': 417,\n",
              " 'pace': 418,\n",
              " 'water': 419,\n",
              " 'adventureoh': 420,\n",
              " 'thank': 421,\n",
              " 'outlet': 422,\n",
              " 'view': 423,\n",
              " 'viewpoint': 424,\n",
              " 'moviesso': 425,\n",
              " 'ole': 426,\n",
              " 'wanna': 427,\n",
              " 'saywould': 428,\n",
              " 'nice': 429,\n",
              " 'read': 430,\n",
              " 'plu': 431,\n",
              " 'huntif': 432,\n",
              " 'rhyme': 433,\n",
              " '10': 434,\n",
              " 'line': 435,\n",
              " 'let': 436,\n",
              " 'submitor': 437,\n",
              " 'leav': 438,\n",
              " 'doubt': 439,\n",
              " 'quitif': 440,\n",
              " 'must': 441,\n",
              " 'origin': 442,\n",
              " 'gut': 443,\n",
              " 'wrench': 444,\n",
              " 'laughter': 445,\n",
              " 'hell': 446,\n",
              " 'mom': 447,\n",
              " 'camp': 448,\n",
              " 'fantast': 449,\n",
              " 'three': 450,\n",
              " 'famou': 451,\n",
              " 'georg': 452,\n",
              " 'clooney': 453,\n",
              " 'im': 454,\n",
              " 'fan': 455,\n",
              " 'roll': 456,\n",
              " 'bad': 457,\n",
              " 'soundtrack': 458,\n",
              " 'man': 459,\n",
              " 'constant': 460,\n",
              " 'sorrow': 461,\n",
              " 'recommand': 462,\n",
              " 'everybodi': 463,\n",
              " 'greet': 464,\n",
              " 'bart': 465,\n",
              " 'simpli': 466,\n",
              " 'remad': 467,\n",
              " 'fail': 468,\n",
              " 'captur': 469,\n",
              " 'flavor': 470,\n",
              " 'terror': 471,\n",
              " '1963': 472,\n",
              " 'titl': 473,\n",
              " 'liam': 474,\n",
              " 'neeson': 475,\n",
              " 'excel': 476,\n",
              " 'alway': 477,\n",
              " 'hold': 478,\n",
              " 'except': 479,\n",
              " 'owen': 480,\n",
              " 'wilson': 481,\n",
              " 'feel': 482,\n",
              " 'luke': 483,\n",
              " 'major': 484,\n",
              " 'fault': 485,\n",
              " 'version': 486,\n",
              " 'stray': 487,\n",
              " 'shirley': 488,\n",
              " 'jackson': 489,\n",
              " 'attempt': 490,\n",
              " 'grandios': 491,\n",
              " 'lost': 492,\n",
              " 'thrill': 493,\n",
              " 'earlier': 494,\n",
              " 'trade': 495,\n",
              " 'snazzier': 496,\n",
              " 'special': 497,\n",
              " 'effect': 498,\n",
              " 'enjoy': 499,\n",
              " 'friction': 500,\n",
              " 'older': 501,\n",
              " 'much': 502,\n",
              " 'rememb': 503,\n",
              " 'filmit': 504,\n",
              " 'cinema': 505,\n",
              " 'dark': 506,\n",
              " 'nervou': 507,\n",
              " '7475': 508,\n",
              " 'dad': 509,\n",
              " 'took': 510,\n",
              " 'brother': 511,\n",
              " 'sister': 512,\n",
              " 'newburi': 513,\n",
              " 'berkshir': 514,\n",
              " 'england': 515,\n",
              " 'recal': 516,\n",
              " 'tiger': 517,\n",
              " 'lot': 518,\n",
              " 'snow': 519,\n",
              " 'also': 520,\n",
              " 'appear': 521,\n",
              " 'grizzli': 522,\n",
              " 'adam': 523,\n",
              " 'dan': 524,\n",
              " 'haggeri': 525,\n",
              " 'think': 526,\n",
              " 'shot': 527,\n",
              " 'die': 528,\n",
              " 'anyon': 529,\n",
              " 'dvd': 530,\n",
              " 'etc': 531,\n",
              " 'pleas': 532,\n",
              " 'knowth': 533,\n",
              " 'fit': 534,\n",
              " 'club': 535,\n",
              " 'shame': 536,\n",
              " 'nearest': 537,\n",
              " '20': 538,\n",
              " 'mile': 539,\n",
              " 'hear': 540,\n",
              " 'other': 541,\n",
              " 'hard': 542,\n",
              " 'sequel': 543,\n",
              " 'surpris': 544,\n",
              " '1990': 545,\n",
              " 'glut': 546,\n",
              " 'cash': 547,\n",
              " 'wrong': 548,\n",
              " 'guy': 549,\n",
              " 'concept': 550,\n",
              " 'cliffhang': 551,\n",
              " 'mountain': 552,\n",
              " 'rescu': 553,\n",
              " 'sli': 554,\n",
              " 'stop': 555,\n",
              " 'shoot': 556,\n",
              " 'stallon': 557,\n",
              " 'nitpick': 558,\n",
              " 'especi': 559,\n",
              " 'expert': 560,\n",
              " 'climb': 561,\n",
              " 'basejump': 562,\n",
              " 'aviat': 563,\n",
              " 'facial': 564,\n",
              " 'express': 565,\n",
              " 'full': 566,\n",
              " 'excus': 567,\n",
              " 'dismiss': 568,\n",
              " 'overblown': 569,\n",
              " 'pile': 570,\n",
              " 'junk': 571,\n",
              " 'outact': 572,\n",
              " 'hors': 573,\n",
              " 'howev': 574,\n",
              " 'nonsens': 575,\n",
              " 'actual': 576,\n",
              " 'lovabl': 577,\n",
              " 'undeni': 578,\n",
              " 'entertain': 579,\n",
              " 'romp': 580,\n",
              " 'deliv': 581,\n",
              " 'plenti': 582,\n",
              " 'unintent': 583,\n",
              " 'youv': 584,\n",
              " 'john': 585,\n",
              " 'lithgow': 586,\n",
              " 'sneeri': 587,\n",
              " 'evil': 588,\n",
              " 'tick': 589,\n",
              " 'box': 590,\n",
              " 'band': 591,\n",
              " 'baddi': 592,\n",
              " 'perman': 593,\n",
              " 'harass': 594,\n",
              " 'hapless': 595,\n",
              " 'turncoat': 596,\n",
              " 'agent': 597,\n",
              " 'rex': 598,\n",
              " 'linn': 599,\n",
              " 'traver': 600,\n",
              " 'henri': 601,\n",
              " 'rooker': 602,\n",
              " 'noteworthi': 603,\n",
              " 'cringeworthi': 604,\n",
              " 'hal': 605,\n",
              " 'insist': 606,\n",
              " 'constantli': 607,\n",
              " 'shriek': 608,\n",
              " 'pain': 609,\n",
              " 'disbelief': 610,\n",
              " 'captor': 611,\n",
              " 'hurt': 612,\n",
              " 'anybodi': 613,\n",
              " 'whilst': 614,\n",
              " 'cant': 615,\n",
              " 'ralph': 616,\n",
              " 'wait': 617,\n",
              " 'frank': 618,\n",
              " 'grin': 619,\n",
              " 'girl': 620,\n",
              " 'plummet': 621,\n",
              " 'former': 622,\n",
              " 'london': 623,\n",
              " 'burn': 624,\n",
              " 'craig': 625,\n",
              " 'fairbrass': 626,\n",
              " 'brit': 627,\n",
              " 'cropper': 628,\n",
              " 'footbal': 629,\n",
              " 'ye': 630,\n",
              " 'help': 631,\n",
              " 'bit': 632,\n",
              " 'kick': 633,\n",
              " 'better': 634,\n",
              " 'judgement': 635,\n",
              " 'care': 636,\n",
              " 'could': 637,\n",
              " 'lower': 638,\n",
              " 'expect': 639,\n",
              " 'volum': 640,\n",
              " 'your': 641,\n",
              " 'qaulen': 642,\n",
              " 'he': 643,\n",
              " 'helicopt': 644,\n",
              " 'absolut': 645,\n",
              " 'hr': 646,\n",
              " 'wont': 647,\n",
              " 'regret': 648,\n",
              " 'rajnikanth': 649,\n",
              " 'carri': 650,\n",
              " 'shoulder': 651,\n",
              " 'although': 652,\n",
              " 'isnt': 653,\n",
              " 'anyth': 654,\n",
              " 'music': 655,\n",
              " 'arrehman': 656,\n",
              " 'take': 657,\n",
              " 'grow': 658,\n",
              " 'heard': 659,\n",
              " 'start': 660,\n",
              " 'karen': 661,\n",
              " 'carpent': 662,\n",
              " 'singer': 663,\n",
              " 'complex': 664,\n",
              " 'though': 665,\n",
              " 'accur': 666,\n",
              " 'detail': 667,\n",
              " 'cynthia': 668,\n",
              " 'gibb': 669,\n",
              " 'portray': 670,\n",
              " 'fine': 671,\n",
              " 'elect': 672,\n",
              " 'actress': 673,\n",
              " 'naiv': 674,\n",
              " 'sort': 675,\n",
              " 'dumb': 676,\n",
              " 'stronger': 677,\n",
              " 'someon': 678,\n",
              " 'louis': 679,\n",
              " 'fletcher': 680,\n",
              " 'agn': 681,\n",
              " 'terrif': 682,\n",
              " 'job': 683,\n",
              " 'song': 684,\n",
              " 'includ': 685,\n",
              " 'album': 686,\n",
              " 'unfortun': 687,\n",
              " 'werent': 688,\n",
              " 'top': 689,\n",
              " 'rate': 690,\n",
              " 'usa': 691,\n",
              " 'sever': 692,\n",
              " 'countri': 693,\n",
              " 'exot': 694,\n",
              " 'masterpiec': 695,\n",
              " 'dizzi': 696,\n",
              " 'trip': 697,\n",
              " 'vast': 698,\n",
              " 'mind': 699,\n",
              " 'conclus': 700,\n",
              " 'evid': 701,\n",
              " 'achiev': 702,\n",
              " 'be': 703,\n",
              " 'unleash': 704,\n",
              " 'uninhibit': 705,\n",
              " 'imagin': 706,\n",
              " 'bold': 707,\n",
              " 'push': 708,\n",
              " 'asid': 709,\n",
              " 'fall': 710,\n",
              " 'formula': 711,\n",
              " 'clich': 712,\n",
              " 'creat': 713,\n",
              " 'someth': 714,\n",
              " 'magnific': 715,\n",
              " 'numer': 716,\n",
              " 'complaint': 717,\n",
              " 'anywher': 718,\n",
              " 'substanc': 719,\n",
              " 'poorli': 720,\n",
              " 'neg': 721,\n",
              " 'critic': 722,\n",
              " 'miss': 723,\n",
              " 'landmark': 724,\n",
              " 'futur': 725,\n",
              " 'hope': 726,\n",
              " 'follow': 727,\n",
              " 'open': 728,\n",
              " 'door': 729,\n",
              " 'slam': 730,\n",
              " 'tarsem': 731,\n",
              " 'singh': 732,\n",
              " 'dont': 733,\n",
              " 'want': 734,\n",
              " 'welcom': 735,\n",
              " 'tri': 736,\n",
              " 'challeng': 737,\n",
              " 'weve': 738,\n",
              " 'talk': 739,\n",
              " 'insid': 740,\n",
              " 'agre': 741,\n",
              " 'genr': 742,\n",
              " 'overwork': 743,\n",
              " '90': 744,\n",
              " 'depict': 745,\n",
              " 'blaze': 746,\n",
              " 'trail': 747,\n",
              " 'twist': 748,\n",
              " 'physic': 749,\n",
              " 'transport': 750,\n",
              " 'noth': 751,\n",
              " 'less': 752,\n",
              " 'fascin': 753,\n",
              " 'journey': 754,\n",
              " 'mysteri': 755,\n",
              " 'subject': 756,\n",
              " 'matter': 757,\n",
              " 'studi': 758,\n",
              " 'bog': 759,\n",
              " 'scientif': 760,\n",
              " 'jargon': 761,\n",
              " 'explain': 762,\n",
              " 'jennif': 763,\n",
              " 'lopez': 764,\n",
              " 'enter': 765,\n",
              " 'brain': 766,\n",
              " 'instead': 767,\n",
              " 'lie': 768,\n",
              " 'laboratori': 769,\n",
              " 'tabl': 770,\n",
              " 'wrap': 771,\n",
              " 'long': 772,\n",
              " 'twizzler': 773,\n",
              " 'jaunt': 774,\n",
              " 'entiti': 775,\n",
              " 'guess': 776,\n",
              " 'that': 777,\n",
              " 'true': 778,\n",
              " 'explan': 779,\n",
              " 'ventur': 780,\n",
              " 'onto': 781,\n",
              " 'ground': 782,\n",
              " 'desir': 783,\n",
              " 'notic': 784,\n",
              " 'realiti': 785,\n",
              " 'mayb': 786,\n",
              " 'contrast': 787,\n",
              " 'bright': 788,\n",
              " 'nonetheless': 789,\n",
              " 'design': 790,\n",
              " 'astonish': 791,\n",
              " 'oscar': 792,\n",
              " 'cinematographi': 793,\n",
              " 'costum': 794,\n",
              " 'itd': 795,\n",
              " 'least': 796,\n",
              " 'nomin': 797,\n",
              " 'kind': 798,\n",
              " 'repeat': 799,\n",
              " 'there': 800,\n",
              " 'els': 801,\n",
              " 'stress': 802,\n",
              " 'enough': 803,\n",
              " 'walk': 804,\n",
              " 'eyepop': 805,\n",
              " 'feast': 806,\n",
              " 'assur': 807,\n",
              " 'write': 808,\n",
              " 'weird': 809,\n",
              " 'crazi': 810,\n",
              " 'psycholog': 811,\n",
              " 'alley': 812,\n",
              " 'member': 813,\n",
              " 'whoever': 814,\n",
              " 'made': 815,\n",
              " 'smoke': 816,\n",
              " '4': 817,\n",
              " 'war': 818,\n",
              " 'hollywood': 819,\n",
              " 'redon': 820,\n",
              " 'clichd': 821,\n",
              " 'rehash': 822,\n",
              " 'overthetop': 823,\n",
              " 'sequenc': 824,\n",
              " 'unavoid': 825,\n",
              " 'conflict': 826,\n",
              " 'largescal': 827,\n",
              " 'combat': 828,\n",
              " 'along': 829,\n",
              " 'grain': 830,\n",
              " 'compel': 831,\n",
              " 'silver': 832,\n",
              " 'screen': 833,\n",
              " 'civil': 834,\n",
              " 'warera': 835,\n",
              " 'cold': 836,\n",
              " 'star': 837,\n",
              " 'jude': 838,\n",
              " 'law': 839,\n",
              " 'nicol': 840,\n",
              " 'kidman': 841,\n",
              " 'rene': 842,\n",
              " 'zellweg': 843,\n",
              " 'quit': 844,\n",
              " 'liter': 845,\n",
              " 'quickanddirti': 846,\n",
              " 'battl': 847,\n",
              " 'put': 848,\n",
              " 'glori': 849,\n",
              " 'edward': 850,\n",
              " 'zwick': 851,\n",
              " 'period': 852,\n",
              " 'center': 853,\n",
              " 'disgruntl': 854,\n",
              " 'confeder': 855,\n",
              " 'soldier': 856,\n",
              " 'inman': 857,\n",
              " 'disgust': 858,\n",
              " 'gruesom': 859,\n",
              " 'homesick': 860,\n",
              " 'beauti': 861,\n",
              " 'hamlet': 862,\n",
              " 'north': 863,\n",
              " 'carolina': 864,\n",
              " 'equal': 865,\n",
              " 'southern': 866,\n",
              " 'bell': 867,\n",
              " 'left': 868,\n",
              " 'behind': 869,\n",
              " 'ada': 870,\n",
              " 'monro': 871,\n",
              " 'glanc': 872,\n",
              " 'setup': 873,\n",
              " 'romant': 874,\n",
              " 'sympathi': 875,\n",
              " 'root': 876,\n",
              " 'reluct': 877,\n",
              " 'tribul': 878,\n",
              " 'battlefield': 879,\n",
              " 'inde': 880,\n",
              " 'segment': 881,\n",
              " 'rel': 882,\n",
              " 'unimpress': 883,\n",
              " 'somewhat': 884,\n",
              " 'contriv': 885,\n",
              " 'soon': 886,\n",
              " 'drastic': 887,\n",
              " 'intrepid': 888,\n",
              " 'desert': 889,\n",
              " 'incident': 890,\n",
              " 'save': 891,\n",
              " 'potenti': 892,\n",
              " 'confus': 893,\n",
              " 'scenario': 894,\n",
              " 'begin': 895,\n",
              " 'odyssey': 896,\n",
              " 'homeward': 897,\n",
              " 'meanwhil': 898,\n",
              " 'farm': 899,\n",
              " 'cultur': 900,\n",
              " 'prove': 901,\n",
              " 'field': 902,\n",
              " 'transform': 903,\n",
              " 'wilderbeast': 904,\n",
              " 'cours': 905,\n",
              " 'toughasnail': 906,\n",
              " 'rubi': 907,\n",
              " 'thew': 908,\n",
              " 'togeth': 909,\n",
              " 'perhap': 910,\n",
              " 'importantli': 911,\n",
              " 'cope': 912,\n",
              " 'isol': 913,\n",
              " 'brought': 914,\n",
              " 'upon': 915,\n",
              " 'within': 916,\n",
              " 'two': 917,\n",
              " 'disturb': 918,\n",
              " 'wartorn': 919,\n",
              " 'south': 920,\n",
              " 'unfold': 921,\n",
              " 'interact': 922,\n",
              " 'surprisingli': 923,\n",
              " 'enhanc': 924,\n",
              " 'brendan': 925,\n",
              " 'gleeson': 926,\n",
              " 'deadbeat': 927,\n",
              " 'father': 928,\n",
              " 'ray': 929,\n",
              " 'winston': 930,\n",
              " 'unrepent': 931,\n",
              " 'lawman': 932,\n",
              " 'natali': 933,\n",
              " 'portman': 934,\n",
              " 'deepli': 935,\n",
              " 'troubl': 936,\n",
              " 'greatli': 937,\n",
              " 'affect': 938,\n",
              " 'northern': 939,\n",
              " 'aggress': 940,\n",
              " 'mostli': 941,\n",
              " 'wors': 942,\n",
              " 'pervad': 943,\n",
              " 'antiwar': 944,\n",
              " 'messag': 945,\n",
              " 'accent': 946,\n",
              " 'haunt': 947,\n",
              " 'score': 948,\n",
              " 'chillingli': 949,\n",
              " 'virginia': 950,\n",
              " 'commun': 951,\n",
              " 'scar': 952,\n",
              " 'land': 953,\n",
              " 'traumat': 954,\n",
              " 'fought': 955,\n",
              " 'weapon': 956,\n",
              " 'tactic': 957,\n",
              " 'past': 958,\n",
              " 'centuri': 959,\n",
              " 'hellish': 960,\n",
              " 'timelessli': 961,\n",
              " 'relev': 962,\n",
              " 'anthoni': 963,\n",
              " 'minghella': 964,\n",
              " 'maintain': 965,\n",
              " 'gloomi': 966,\n",
              " 'mood': 967,\n",
              " 'atmospher': 968,\n",
              " 'denigr': 969,\n",
              " 'tepid': 970,\n",
              " 'climax': 971,\n",
              " 'justic': 972,\n",
              " 'form': 973,\n",
              " 'awkwardli': 974,\n",
              " 'tack': 975,\n",
              " 'end': 976,\n",
              " 'inher': 977,\n",
              " 'distant': 978,\n",
              " 'abstract': 979,\n",
              " 'absurd': 980,\n",
              " 'natur': 981,\n",
              " 'relationship': 982,\n",
              " 'dismal': 983,\n",
              " 'mistak': 984,\n",
              " 'neither': 985,\n",
              " 'trait': 986,\n",
              " 'feelgood': 987,\n",
              " 'romanc': 988,\n",
              " 'inspir': 989,\n",
              " 'drama': 990,\n",
              " 'uniqu': 991,\n",
              " 'vision': 992,\n",
              " 'era': 993,\n",
              " 'absorb': 994,\n",
              " 'torn': 995,\n",
              " 'apart': 996,\n",
              " 'desper': 997,\n",
              " 'rid': 998,\n",
              " 'repercuss': 999,\n",
              " ...}"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3 - Converting a review to a Tensor\n",
        "\n",
        "\n",
        "*   This function returns a list of integers representing the processed moview review.\n",
        "*   Words not present in the dictionary are replaced with \"__UNK__\"\n",
        "\n"
      ],
      "metadata": {
        "id": "Lh_NAGFILGFY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Convert the reviews to a tensor ###\n",
        "\n",
        "def convert_to_tensor(review, vocab_dict, unk_token='__UNK__', verbose=False):\n",
        "    '''\n",
        "    Input:\n",
        "        review - A string containing the review\n",
        "        vocab_dict - The words dictionary\n",
        "        unk_token - The special string for unknown tokens\n",
        "        verbose - Print info durign runtime\n",
        "    Output:\n",
        "        tensor_l - A python list with\n",
        "\n",
        "    '''\n",
        "\n",
        "    # Process the review into a list of words\n",
        "    # where only important words are kept (stop words removed)\n",
        "    word_l = process_review(review)\n",
        "\n",
        "    if verbose:\n",
        "        print(\"List of words from the processed review:\")\n",
        "        print(word_l)\n",
        "\n",
        "    # Initialize the list that will contain the unique integer IDs of each word\n",
        "    tensor_l = []\n",
        "\n",
        "    # Get the unique integer ID of the __UNK__ token\n",
        "    unk_ID = vocab_dict[unk_token]\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"The unique integer ID for the unk_token is {unk_ID}\")\n",
        "\n",
        "    # for each word in the list:\n",
        "    for word in word_l:\n",
        "\n",
        "        # Get the unique integer ID.\n",
        "        # If the word doesn't exist in the vocab dictionary,\n",
        "        # use the unique ID for __UNK__ instead.\n",
        "        word_ID = vocab_dict[word] if word in vocab_dict.keys() else unk_ID\n",
        "\n",
        "        # Append the unique integer ID to the tensor list.\n",
        "        tensor_l.append(word_ID)\n",
        "\n",
        "\n",
        "    return tensor_l"
      ],
      "metadata": {
        "id": "HSxZ021MZrqj"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Actual review is\\n\", val_pos[0])\n",
        "print(\"\\nTensor of review:\\n\", convert_to_tensor(val_pos[0], vocab_dict=Vocab))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CeKGF5K4Z0GS",
        "outputId": "ce1d88a0-f630-468a-d5e6-e2d668491d4a"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Actual review is\n",
            " The finest short I've ever seen. Some commentators suggest it might have been lengthened, due to the density of insight it offers. There's irony in that comment and little merit. The acting is all up to Noonan and he carries his thankless character perfectly. I might have preferred that the narrator be less \"recognizable\", but the gravitas lent is pitch perfect. This is a short for people who read, for those whose \"bar\" is set high and for those who recognize that living in a culture that celebrates stupidity and banality can forge contrary and bitter defenders of beauty. A beautiful short film. FWIW: I was pleased at the Picasso reference, since I once believed that Picasso was just another art whore with little talent; like, I assume, most people - until the day I saw some drawings he made when he was 12. Picasso was a finer draftsman and a brilliant artist at that age than many artists will ever become in a lifetime. I understood immediately why he had to make the art he became known for.\n",
            "\n",
            "Tensor of review:\n",
            " [1153, 1228, 248, 101, 364, 3324, 1527, 2066, 16123, 85, 27751, 3250, 277, 800, 3221, 3324, 142, 4093, 329, 29549, 650, 27578, 219, 2665, 2066, 3813, 1538, 752, 5158, 28755, 5452, 8483, 1948, 1228, 287, 430, 1653, 1019, 22, 59, 3679, 314, 900, 2497, 3328, 9550, 13134, 3501, 1798, 7772, 861, 861, 1228, 144, 47338, 532, 57150, 169, 1139, 390, 57150, 303, 1673, 16264, 142, 342, 380, 4246, 287, 1321, 102, 1052, 815, 2821, 57150, 13361, 2, 2045, 2440, 1858, 240, 2440, 101, 134, 4504, 5999, 1190, 344, 1673, 2342, 1999]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.4. Creating the batch generator\n",
        "This function takes in the positive/negative tweets and returns a batch of training examples. It returns the model inputs, the targets (positive or negative labels) and the weight for each target."
      ],
      "metadata": {
        "id": "JcDiQB3PL9aL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def data_generator(data_pos, data_neg, batch_size, loop, vocab_dict, shuffle=False):\n",
        "    '''\n",
        "    Input:\n",
        "        data_pos - Set of positive examples\n",
        "        data_neg - Set of negative examples\n",
        "        batch_size - number of samples per batch. Must be even\n",
        "        loop - True or False\n",
        "        vocab_dict - The words dictionary\n",
        "        shuffle - Shuffle the data order\n",
        "    Yield:\n",
        "        inputs - Subset of positive and negative examples\n",
        "        targets - The corresponding labels for the subset\n",
        "        example_weights - A numpy array specifying the importance of each example\n",
        "\n",
        "    '''\n",
        "\n",
        "    # make sure the batch size is an even number\n",
        "    # to allow an equal number of positive and negative samples\n",
        "    assert batch_size % 2 == 0\n",
        "\n",
        "    # Number of positive examples in each batch is half of the batch size\n",
        "    # same with number of negative examples in each batch\n",
        "    n_to_take = batch_size // 2\n",
        "\n",
        "    # Use pos_index to walk through the data_pos array\n",
        "    # same with neg_index and data_neg\n",
        "    pos_index = 0\n",
        "    neg_index = 0\n",
        "\n",
        "    len_data_pos = len(data_pos)\n",
        "    len_data_neg = len(data_neg)\n",
        "\n",
        "    # Get and array with the data indexes\n",
        "    pos_index_lines = list(range(len_data_pos))\n",
        "    neg_index_lines = list(range(len_data_neg))\n",
        "\n",
        "    # shuffle lines if shuffle is set to True\n",
        "    if shuffle:\n",
        "        rnd.shuffle(pos_index_lines)\n",
        "        rnd.shuffle(neg_index_lines)\n",
        "\n",
        "    stop = False\n",
        "\n",
        "    # Loop indefinitely\n",
        "    while not stop:\n",
        "\n",
        "        # create a batch with positive and negative examples\n",
        "        batch = []\n",
        "\n",
        "        # First part: Pack n_to_take positive examples\n",
        "\n",
        "        # Start from 0 and increment i up to n_to_take\n",
        "        for i in range(n_to_take):\n",
        "\n",
        "            # If the positive index goes past the positive dataset,\n",
        "            if pos_index >= len_data_pos:\n",
        "\n",
        "                # If loop is set to False, break once we reach the end of the dataset\n",
        "                if not loop:\n",
        "                    stop = True;\n",
        "                    break;\n",
        "                # If user wants to keep re-using the data, reset the index\n",
        "                pos_index = 0\n",
        "                if shuffle:\n",
        "                    # Shuffle the index of the positive sample\n",
        "                    rnd.shuffle(pos_index_lines)\n",
        "\n",
        "            # get the tweet as pos_index\n",
        "            review = data_pos[pos_index_lines[pos_index]]\n",
        "\n",
        "            # convert the review into tensors of integers representing the processed words\n",
        "            tensor = convert_to_tensor(review, vocab_dict)\n",
        "\n",
        "            # append the tensor to the batch list\n",
        "            batch.append(tensor)\n",
        "\n",
        "            # Increment pos_index by one\n",
        "            pos_index = pos_index + 1\n",
        "\n",
        "        # Second part: Pack n_to_take negative examples\n",
        "\n",
        "        # Using the same batch list, start from 0 and increment i up to n_to_take\n",
        "        for i in range(n_to_take):\n",
        "\n",
        "            # If the negative index goes past the negative dataset,\n",
        "            if neg_index >= len_data_neg:\n",
        "\n",
        "                # If loop is set to False, break once we reach the end of the dataset\n",
        "                if not loop:\n",
        "                    stop = True\n",
        "                    break\n",
        "\n",
        "                # If user wants to keep re-using the data, reset the index\n",
        "                neg_index = 0\n",
        "\n",
        "                if shuffle:\n",
        "                    # Shuffle the index of the negative sample\n",
        "                    rnd.shuffle(neg_index_lines)\n",
        "\n",
        "            # get the tweet as neg_index\n",
        "            review = data_neg[neg_index_lines[neg_index]]\n",
        "\n",
        "            # convert the review into tensors of integers representing the processed words\n",
        "            tensor = convert_to_tensor(review, vocab_dict)\n",
        "\n",
        "            # append the tensor to the batch list\n",
        "            batch.append(tensor)\n",
        "\n",
        "            # Increment neg_index by one\n",
        "            neg_index = neg_index + 1\n",
        "\n",
        "        ### END CODE HERE ###\n",
        "\n",
        "        if stop:\n",
        "            break;\n",
        "\n",
        "        # Get the max tweet length (the length of the longest tweet)\n",
        "        # (you will pad all shorter tweets to have this length)\n",
        "        max_len = max([len(t) for t in batch])\n",
        "\n",
        "\n",
        "        # Initialize the input_l, which will\n",
        "        # store the padded versions of the tensors\n",
        "        tensor_pad_l = []\n",
        "        # Pad shorter tweets with zeros\n",
        "        for tensor in batch:\n",
        "\n",
        "\n",
        "            # Get the number of positions to pad for this tensor so that it will be max_len long\n",
        "            n_pad = max_len - len(tensor)\n",
        "\n",
        "            # Generate a list of zeros, with length n_pad\n",
        "            pad_l = [0] * n_pad\n",
        "\n",
        "            # concatenate the tensor and the list of padded zeros\n",
        "            tensor_pad = tensor + pad_l\n",
        "\n",
        "            # append the padded tensor to the list of padded tensors\n",
        "            tensor_pad_l.append(tensor_pad)\n",
        "\n",
        "        # convert the list of padded tensors to a numpy array\n",
        "        # and store this as the model inputs\n",
        "        inputs = np.array(tensor_pad_l)\n",
        "\n",
        "        # Generate the list of targets for the positive examples (a list of ones)\n",
        "        # The length is the number of positive examples in the batch\n",
        "        target_pos = [1] *n_to_take\n",
        "\n",
        "        # Generate the list of targets for the negative examples (a list of zeros)\n",
        "        # The length is the number of negative examples in the batch\n",
        "        target_neg = [0] *n_to_take\n",
        "\n",
        "        # Concatenate the positve and negative targets\n",
        "        target_l = target_pos + target_neg\n",
        "\n",
        "        # Convert the target list into a numpy array\n",
        "        targets = np.array(target_l)\n",
        "\n",
        "        # Example weights: Treat all examples equally importantly.\n",
        "        example_weights = np.ones_like(targets)\n",
        "\n",
        "\n",
        "\n",
        "        # note we use yield and not return\n",
        "        yield inputs, targets, example_weights"
      ],
      "metadata": {
        "id": "ivpmv5xAZ3DR"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the random number generator for the shuffle procedure\n",
        "rnd.seed(30)\n",
        "\n",
        "# Create the training data generator\n",
        "\n",
        "def train_generator(batch_size, train_pos\n",
        "                    , train_neg, vocab_dict, loop=True\n",
        "                    , shuffle = False):\n",
        "    return data_generator(train_pos, train_neg, batch_size, loop, vocab_dict, shuffle)\n",
        "\n",
        "# Create the validation data generator\n",
        "def val_generator(batch_size, val_pos\n",
        "                    , val_neg, vocab_dict, loop=True\n",
        "                    , shuffle = False):\n",
        "    return data_generator(val_pos, val_neg, batch_size, loop, vocab_dict, shuffle)\n",
        "\n",
        "# Create the validation data generator\n",
        "def test_generator(batch_size, test_pos\n",
        "                    , test_neg, vocab_dict, loop=False\n",
        "                    , shuffle = False):\n",
        "    return data_generator(test_pos, test_neg, batch_size, loop, vocab_dict, shuffle)\n",
        "\n",
        "# Get a batch from the train_generator and inspect.\n",
        "inputs, targets, example_weights = next(train_generator(4, train_pos, train_neg, Vocab, shuffle=True))\n",
        "\n",
        "# this will print a list of 4 tensors padded with zeros\n",
        "print(f'Inputs: {inputs}')\n",
        "print(f'Targets: {targets}')\n",
        "print(f'Example Weights: {example_weights}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ym0pF0_Z5qS",
        "outputId": "f358e826-b51f-4008-a784-aafb6a58b31d"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inputs: [[  282   187   699   246     3   322  2577   282   755   248   101   364\n",
            "    329  2536   248   364  3656   236  2714  1237   244   243  2765   190\n",
            "   3007  1237   268  2558  2765   282  1201   324 46236   141   529  1527\n",
            "    324   248   736  7273 22113  7446  3402  1803  1200   248   364  8889\n",
            "     36   282  1463   282    36   221   848 17238   951  1883   529  2765\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0]\n",
            " [  529    98   526  3020  5393 19371   754     3  1405   282   206   416\n",
            "   1569  2115  3017   252  1803 14932 19702   520  2961   306  2001   206\n",
            "      3  2300   144   576  1612   915  4443 19371   754   499  2061  1719\n",
            "    685 17947    72   976  1686  1244 15256   734    90  6919  1413   122\n",
            "    322  5393  7325 12620  3020  5393  1648     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0]\n",
            " [  615   631  1803  2168  8494   380  1842   221  2126   736 12106   808\n",
            "    330   179   822  1042   690  3696   726  2657  1973  8424  6237   129\n",
            "    199   204   344    16   942  1160   459  3449  2213   593   454  2547\n",
            "   1809   619    56    81  5981   581   179  2968    98   768   751   380\n",
            "    321 10391  4640   657   982  7488  3361 22311  1181  5011  1604  2056\n",
            "    408    26   181  9996 91371  1788 39436   205  1504    26  2453 15303\n",
            "    221   100  3191  6067  7484  5653   245    26  4213  3363   376   618\n",
            "  39668  7486   206  3460   941 10461   419    85 15254  1063  6038  3679\n",
            "   3696   221 14486     9  4967   829  9130 10808   683  4737   751   922\n",
            "    221   942   221  3313   342   342 70459  3102   889]\n",
            " [  201   144   354   435  7214   689   105   762  3166  2109   330  1237\n",
            "   3978  2622  1094   295  3967  1609  1812  6965  2622     6   144   380\n",
            "    733    82  1842  3065   917    16   637 10472    82   735  1842  4589\n",
            "   2465   276  1430  1842  1500   376  1038    16  5923  3062 10472  1430\n",
            "   4592  1507   848  3558 27787   376   733   380  3062   714   751   354\n",
            "    435  7214  1228   457   354    88  3313   206   346   244     6  6950\n",
            "    767     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0]]\n",
            "Targets: [1 1 0 0]\n",
            "Example Weights: [1 1 1 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3 - Model Creation"
      ],
      "metadata": {
        "id": "yyNMNfvsMgNr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install trax"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MPXhTKbqaBMM",
        "outputId": "6c19854c-c14a-4af7-d8f1-fe863c880b45"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: trax in /usr/local/lib/python3.10/dist-packages (1.4.1)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from trax) (1.4.0)\n",
            "Requirement already satisfied: funcsigs in /usr/local/lib/python3.10/dist-packages (from trax) (1.0.2)\n",
            "Requirement already satisfied: gin-config in /usr/local/lib/python3.10/dist-packages (from trax) (0.5.0)\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.10/dist-packages (from trax) (0.25.2)\n",
            "Requirement already satisfied: jax in /usr/local/lib/python3.10/dist-packages (from trax) (0.4.14)\n",
            "Requirement already satisfied: jaxlib in /usr/local/lib/python3.10/dist-packages (from trax) (0.4.14+cuda11.cudnn86)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from trax) (3.7.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from trax) (1.23.5)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from trax) (5.9.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from trax) (1.11.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from trax) (1.16.0)\n",
            "Requirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.10/dist-packages (from trax) (4.9.2)\n",
            "Requirement already satisfied: tensorflow-text in /usr/local/lib/python3.10/dist-packages (from trax) (2.13.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym->trax) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym->trax) (0.0.8)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from jax->trax) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.10/dist-packages (from jax->trax) (3.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->trax) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->trax) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->trax) (4.42.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->trax) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->trax) (23.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->trax) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->trax) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->trax) (2.8.2)\n",
            "Requirement already satisfied: array-record in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->trax) (0.4.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->trax) (8.1.7)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->trax) (0.1.8)\n",
            "Requirement already satisfied: etils[enp,epath]>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->trax) (1.4.1)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->trax) (2.3)\n",
            "Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->trax) (3.20.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->trax) (2.31.0)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->trax) (1.14.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->trax) (2.3.0)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->trax) (0.10.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->trax) (4.66.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->trax) (1.15.0)\n",
            "Requirement already satisfied: tensorflow-hub>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-text->trax) (0.14.0)\n",
            "Requirement already satisfied: tensorflow<2.14,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-text->trax) (2.13.0)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath]>=0.9.0->tensorflow-datasets->trax) (6.0.1)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath]>=0.9.0->tensorflow-datasets->trax) (4.5.0)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath]>=0.9.0->tensorflow-datasets->trax) (3.16.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->tensorflow-datasets->trax) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->tensorflow-datasets->trax) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->tensorflow-datasets->trax) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->tensorflow-datasets->trax) (2023.7.22)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text->trax) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.1.21 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text->trax) (23.5.26)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text->trax) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text->trax) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text->trax) (1.57.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text->trax) (3.9.0)\n",
            "Requirement already satisfied: keras<2.14,>=2.13.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text->trax) (2.13.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text->trax) (16.0.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text->trax) (67.7.2)\n",
            "Requirement already satisfied: tensorboard<2.14,>=2.13 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text->trax) (2.13.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.14,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text->trax) (2.13.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text->trax) (0.33.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-metadata->tensorflow-datasets->trax) (1.60.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow<2.14,>=2.13.0->tensorflow-text->trax) (0.41.2)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text->trax) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text->trax) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text->trax) (3.4.4)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text->trax) (0.7.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text->trax) (2.3.7)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text->trax) (5.3.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text->trax) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text->trax) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text->trax) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text->trax) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text->trax) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text->trax) (3.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import trax\n",
        "from trax.supervised import training\n",
        "import trax.fastmath.numpy as np\n",
        "from trax import layers as tl\n",
        "from trax import fastmath"
      ],
      "metadata": {
        "id": "9pQbKRQwL5lr"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Layers have weights and a forward function.\n",
        "# They create weights when layer.initialize is called and use them.\n",
        "\n",
        "class Layer(object):\n",
        "    \"\"\"Base class for layers.\"\"\"\n",
        "    def __init__(self):\n",
        "        self.weights = None\n",
        "\n",
        "    # initializes the weights based on the input signature and random key\n",
        "    def forward(self, x):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    # initializes and returns the weights\n",
        "    def init_weights_and_state(self, input_signature, random_key):\n",
        "        pass\n",
        "\n",
        "    def init(self, input_signature, random_key):\n",
        "        self.init_weights_and_state(input_signature, random_key)\n",
        "        return self.weights\n",
        "\n",
        "    def __call__(self, x):\n",
        "        return self.forward(x)\n",
        "\n",
        "\n",
        "class Relu(Layer):\n",
        "    \"\"\"Relu activation function implementation\"\"\"\n",
        "    def forward(self, x):\n",
        "        '''\n",
        "        Input:\n",
        "            - x (a numpy array): the input\n",
        "        Output:\n",
        "            - activation (numpy array): all positive or 0 version of x\n",
        "        '''\n",
        "\n",
        "        activation = np.maximum(x, 0)\n",
        "\n",
        "        return activation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1rI6Mm0OZ_FP",
        "outputId": "e9f92ce0-27ff-479a-edd6-71f0fa72338b"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Dense(Layer):\n",
        "    \"\"\"\n",
        "    A dense (fully-connected) layer.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_units, init_stdev=0.1):\n",
        "\n",
        "        # Set the number of units in this layer\n",
        "        self._n_units = n_units\n",
        "        self._init_stdev = init_stdev\n",
        "\n",
        "    # Matrix multiply x and the weight matrix\n",
        "    def forward(self, x):\n",
        "\n",
        "        dense = np.dot(x, self.weights)\n",
        "\n",
        "        return dense\n",
        "\n",
        "    # init_weights\n",
        "    def init_weights_and_state(self, input_signature, random_key):\n",
        "\n",
        "        # The input_signature has a .shape attribute that gives the shape as a tuple\n",
        "\n",
        "        input_shape = input_signature.shape\n",
        "\n",
        "        # Generate the weight matrix from a normal distribution,\n",
        "        # and standard deviation of 'stdev'\n",
        "        w = 0.1 * trax.fastmath.random.normal(key = random_key, shape = (input_shape[-1], self._n_units))\n",
        "\n",
        "        self.weights = w\n",
        "        return self.weights"
      ],
      "metadata": {
        "id": "V8mwzlciM-xd"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def classifier(vocab_size=102924, embedding_dim=300, output_dim=2, mode='train'):\n",
        "\n",
        "    # create embedding layer\n",
        "    embed_layer = tl.Embedding(\n",
        "        vocab_size=vocab_size, # Size of the vocabulary\n",
        "        d_feature=embedding_dim # Embedding dimension\n",
        "    )\n",
        "\n",
        "    # Create a mean layer, to create an \"average\" word embedding\n",
        "    mean_layer = tl.Mean(axis=1)\n",
        "\n",
        "    # Create a dense layer, one unit for each output\n",
        "    dense_output_layer = tl.Dense(n_units = output_dim)\n",
        "\n",
        "    # Use tl.Serial to combine all layers\n",
        "    # and create the classifier\n",
        "    # of type trax.layers.combinators.Serial\n",
        "    model = tl.Serial(\n",
        "      embed_layer, # embedding layer\n",
        "      mean_layer, # mean layer\n",
        "      dense_output_layer # dense output layer\n",
        "    )\n",
        "\n",
        "    # return the model of type\n",
        "    return model"
      ],
      "metadata": {
        "id": "vT776XSpaHHF"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_train_eval_tasks(train_pos, train_neg, val_pos, val_neg, vocab_dict, loop, batch_size = 16):\n",
        "\n",
        "    rnd.seed(271)\n",
        "\n",
        "    train_task = training.TrainTask(\n",
        "        labeled_data=train_generator(batch_size, train_pos\n",
        "                    , train_neg, vocab_dict, loop\n",
        "                    , shuffle = True),\n",
        "        loss_layer=tl.WeightedCategoryCrossEntropy(),\n",
        "        optimizer=trax.optimizers.Adam(0.01),\n",
        "        n_steps_per_checkpoint=10,\n",
        "    )\n",
        "\n",
        "    eval_task = training.EvalTask(\n",
        "        labeled_data=val_generator(batch_size, val_pos\n",
        "                    , val_neg, vocab_dict, loop\n",
        "                    , shuffle = True),\n",
        "        metrics=[tl.WeightedCategoryCrossEntropy(), tl.WeightedCategoryAccuracy()],\n",
        "    )\n",
        "\n",
        "    return train_task, eval_task"
      ],
      "metadata": {
        "id": "xO0Kq70aaN76"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_task, eval_task = get_train_eval_tasks(train_pos, train_neg, val_pos, val_neg, Vocab, True, batch_size = 16)\n",
        "model = classifier()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q0uSewcFazo-",
        "outputId": "e81bc6c6-6a42-4953-99d6-23e4cdf023dc"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:jax._src.xla_bridge:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(classifier, train_task, eval_task, n_steps, output_dir):\n",
        "    '''\n",
        "    Input:\n",
        "        classifier - the model you are building\n",
        "        train_task - Training task\n",
        "        eval_task - Evaluation task. Received as a list.\n",
        "        n_steps - the evaluation steps\n",
        "        output_dir - folder to save your files\n",
        "    Output:\n",
        "        trainer -  trax trainer\n",
        "    '''\n",
        "    rnd.seed(31)\n",
        "\n",
        "    training_loop = training.Loop(\n",
        "                                classifier, # The learning model\n",
        "                                train_task, # The training task\n",
        "                                eval_tasks=eval_task, # The evaluation task\n",
        "                                output_dir=output_dir, # The output directory\n",
        "                                random_seed=31\n",
        "    )\n",
        "\n",
        "    training_loop.run(n_steps = n_steps)\n",
        "\n",
        "    # Return the training_loop, since it has the model.\n",
        "    return training_loop"
      ],
      "metadata": {
        "id": "7VdfD1didqR2"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dir_path = './model/'\n",
        "\n",
        "try:\n",
        "    shutil.rmtree(dir_path)\n",
        "except OSError as e:\n",
        "    pass\n",
        "\n",
        "\n",
        "output_dir = './model/'\n",
        "output_dir_expand = os.path.expanduser(output_dir)\n",
        "print(output_dir_expand)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HO1Mq3Reg773",
        "outputId": "73b4c012-e3c5-4d59-d2ef-35c50fa66192"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "./model/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_loop = train_model(model, train_task, [eval_task], 100, output_dir_expand)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MAlJaHrcgwJb",
        "outputId": "1a6617ab-5100-4260-c0f4-c8e38aae4a41"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/jax/_src/xla_bridge.py:851: UserWarning: jax.host_count has been renamed to jax.process_count. This alias will eventually be removed; please update your code.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/trax/layers/base.py:851: FutureWarning: GzipFile was opened for writing, but this will change in future Python releases.  Specify the mode argument for opening it for writing.\n",
            "  with gzip.GzipFile(fileobj=f, compresslevel=compresslevel) as gzipf:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step      1: Total number of trainable weights: 30877802\n",
            "Step      1: Ran 1 train steps in 9.88 secs\n",
            "Step      1: train WeightedCategoryCrossEntropy |  0.69318092\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/trax/supervised/training.py:1249: FutureWarning: GzipFile was opened for writing, but this will change in future Python releases.  Specify the mode argument for opening it for writing.\n",
            "  with gzip_lib.GzipFile(fileobj=f, compresslevel=2) as gzipf:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step      1: eval  WeightedCategoryCrossEntropy |  0.69267797\n",
            "Step      1: eval      WeightedCategoryAccuracy |  0.56250000\n",
            "\n",
            "Step     10: Ran 9 train steps in 22.68 secs\n",
            "Step     10: train WeightedCategoryCrossEntropy |  0.69842875\n",
            "Step     10: eval  WeightedCategoryCrossEntropy |  0.69015151\n",
            "Step     10: eval      WeightedCategoryAccuracy |  0.50000000\n",
            "\n",
            "Step     20: Ran 10 train steps in 12.38 secs\n",
            "Step     20: train WeightedCategoryCrossEntropy |  0.69283170\n",
            "Step     20: eval  WeightedCategoryCrossEntropy |  0.68016803\n",
            "Step     20: eval      WeightedCategoryAccuracy |  0.50000000\n",
            "\n",
            "Step     30: Ran 10 train steps in 12.40 secs\n",
            "Step     30: train WeightedCategoryCrossEntropy |  0.67948288\n",
            "Step     30: eval  WeightedCategoryCrossEntropy |  0.64852333\n",
            "Step     30: eval      WeightedCategoryAccuracy |  0.56250000\n",
            "\n",
            "Step     40: Ran 10 train steps in 12.56 secs\n",
            "Step     40: train WeightedCategoryCrossEntropy |  0.66291070\n",
            "Step     40: eval  WeightedCategoryCrossEntropy |  0.64454019\n",
            "Step     40: eval      WeightedCategoryAccuracy |  0.75000000\n",
            "\n",
            "Step     50: Ran 10 train steps in 11.95 secs\n",
            "Step     50: train WeightedCategoryCrossEntropy |  0.64648896\n",
            "Step     50: eval  WeightedCategoryCrossEntropy |  0.64019716\n",
            "Step     50: eval      WeightedCategoryAccuracy |  0.56250000\n",
            "\n",
            "Step     60: Ran 10 train steps in 12.77 secs\n",
            "Step     60: train WeightedCategoryCrossEntropy |  0.59987187\n",
            "Step     60: eval  WeightedCategoryCrossEntropy |  0.58007389\n",
            "Step     60: eval      WeightedCategoryAccuracy |  0.75000000\n",
            "\n",
            "Step     70: Ran 10 train steps in 14.63 secs\n",
            "Step     70: train WeightedCategoryCrossEntropy |  0.54147059\n",
            "Step     70: eval  WeightedCategoryCrossEntropy |  0.50968254\n",
            "Step     70: eval      WeightedCategoryAccuracy |  0.68750000\n",
            "\n",
            "Step     80: Ran 10 train steps in 12.42 secs\n",
            "Step     80: train WeightedCategoryCrossEntropy |  0.50917488\n",
            "Step     80: eval  WeightedCategoryCrossEntropy |  0.43881378\n",
            "Step     80: eval      WeightedCategoryAccuracy |  0.81250000\n",
            "\n",
            "Step     90: Ran 10 train steps in 12.06 secs\n",
            "Step     90: train WeightedCategoryCrossEntropy |  0.49922204\n",
            "Step     90: eval  WeightedCategoryCrossEntropy |  0.48729524\n",
            "Step     90: eval      WeightedCategoryAccuracy |  0.81250000\n",
            "\n",
            "Step    100: Ran 10 train steps in 12.51 secs\n",
            "Step    100: train WeightedCategoryCrossEntropy |  0.49294773\n",
            "Step    100: eval  WeightedCategoryCrossEntropy |  0.52576506\n",
            "Step    100: eval      WeightedCategoryAccuracy |  0.81250000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_accuracy(preds, y, y_weights):\n",
        "    \"\"\"\n",
        "    Input:\n",
        "        preds: a tensor of shape (dim_batch, output_dim)\n",
        "        y: a tensor of shape (dim_batch,) with the true labels\n",
        "        y_weights: a n.ndarray with the a weight for each example\n",
        "    Output:\n",
        "        accuracy: a float between 0-1\n",
        "        weighted_num_correct (np.float32): Sum of the weighted correct predictions\n",
        "        sum_weights (np.float32): Sum of the weights\n",
        "    \"\"\"\n",
        "    # Create an array of booleans,\n",
        "    # True if the probability of positive sentiment is greater than\n",
        "    # the probability of negative sentiment\n",
        "    # else False\n",
        "    is_pos = preds[:, 1] > preds[:, 0]\n",
        "\n",
        "    # convert the array of booleans into an array of np.int32\n",
        "    is_pos_int = is_pos.astype(np.int32)\n",
        "\n",
        "    # compare the array of predictions (as int32) with the target (labels) of type int32\n",
        "    correct = is_pos_int == y\n",
        "\n",
        "    # Count the sum of the weights.\n",
        "    sum_weights = np.sum(y_weights)\n",
        "\n",
        "    # convert the array of correct predictions (boolean) into an arrayof np.float32\n",
        "    correct_float = correct.astype(np.float32)\n",
        "\n",
        "    # Multiply each prediction with its corresponding weight.\n",
        "    weighted_correct_float = correct_float * y_weights\n",
        "\n",
        "    # Sum up the weighted correct predictions (of type np.float32), to go in the\n",
        "    # numerator.\n",
        "    weighted_num_correct = np.sum(weighted_correct_float)\n",
        "\n",
        "    # Divide the number of weighted correct predictions by the sum of the\n",
        "    # weights.\n",
        "    accuracy = weighted_num_correct / sum_weights\n",
        "\n",
        "    return accuracy, weighted_num_correct, sum_weights"
      ],
      "metadata": {
        "id": "iReZQEa3wNd5"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_model(generator, model, compute_accuracy=compute_accuracy):\n",
        "    '''\n",
        "    Input:\n",
        "        generator: an iterator instance that provides batches of inputs and targets\n",
        "        model: a model instance\n",
        "    Output:\n",
        "        accuracy: float corresponding to the accuracy\n",
        "    '''\n",
        "\n",
        "    accuracy = 0\n",
        "    total_num_correct = 0\n",
        "    total_num_pred = 0\n",
        "\n",
        "    all_pred = []\n",
        "    all_targets = []\n",
        "    batch_size = []\n",
        "\n",
        "\n",
        "    for batch in generator:\n",
        "        batch_size.append(len(batch[0]))\n",
        "        # Retrieve the inputs from the batch\n",
        "        inputs =  batch[0]\n",
        "\n",
        "        # Retrieve the targets (actual labels) from the batch\n",
        "        targets = batch[1]\n",
        "        all_targets.append(targets)\n",
        "\n",
        "        # Retrieve the example weight.\n",
        "        example_weight = batch[2]\n",
        "\n",
        "        # Make predictions using the inputs\n",
        "        pred = model(inputs)\n",
        "        all_pred.append(pred)\n",
        "\n",
        "        # Calculate accuracy for the batch by comparing its predictions and targets\n",
        "        batch_accuracy, batch_num_correct, batch_num_pred = compute_accuracy(pred, targets, example_weight)\n",
        "\n",
        "        # Update the total number of correct predictions\n",
        "        # by adding the number of correct predictions from this batch\n",
        "        total_num_correct += batch_num_correct\n",
        "\n",
        "        # Update the total number of predictions\n",
        "        # by adding the number of predictions made for the batch\n",
        "        total_num_pred += batch_num_pred\n",
        "\n",
        "    # Calculate accuracy over all examples\n",
        "    accuracy = total_num_correct / total_num_pred\n",
        "\n",
        "\n",
        "    return all_pred, all_targets, batch_size, accuracy"
      ],
      "metadata": {
        "id": "rSLChmk_wWFZ"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_pred, train_targets, train_batch_size, train_accuracy  = test_model(train_generator(10, train_pos\n",
        "                    , train_neg, Vocab, loop=False\n",
        "                    , shuffle = False), model)"
      ],
      "metadata": {
        "id": "tomyPNxzFRZJ"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"The model's accuracy on the training set is {train_accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bieCiY9SHMHn",
        "outputId": "5d0ebe5e-07b0-43d9-fd0e-1e82be1eace9"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model's accuracy on the training set is 0.7722880840301514\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = training_loop.eval_model\n",
        "pred, targets, batch_size, accuracy = test_model(test_generator(10, test_pos\n",
        "                    , test_neg, Vocab, loop=False\n",
        "                    , shuffle = False), model)\n",
        "\n",
        "print(f'The accuracy of your model on the test set is {accuracy:.4f}', )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ImbWgY1pvIjb",
        "outputId": "94354e95-00d3-4332-f93d-d9b33241abc6"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The accuracy of your model on the test set is 0.7613\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "binary_pred = []\n",
        "for array in pred:\n",
        "    tmp_is_positive = array[:, 1] > array[:, 0]\n",
        "    tmp_is_positive_01 = [1 if val else 0 for val in tmp_is_positive]\n",
        "    binary_pred = binary_pred + tmp_is_positive_01"
      ],
      "metadata": {
        "id": "rOm_gzeV0tnY"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_targets = [item for sublist in targets for item in sublist]"
      ],
      "metadata": {
        "id": "cfssi5Dy4J5u"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_set = pd.DataFrame({\n",
        "    'review': test_x[:7410],\n",
        "    'pred': binary_pred,\n",
        "    'actual': test_targets\n",
        "})"
      ],
      "metadata": {
        "id": "R_yEklvA4xew"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_set.head"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Ya4ZMMG9onm",
        "outputId": "a67a9b0b-c5c6-4b88-c05f-4cf25247e4bc"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method NDFrame.head of                                                  review  pred actual\n",
              "0     Count me as being one who is happy to see no H...     0      1\n",
              "1     I know, that's not what you expect from a film...     1      1\n",
              "2     I don't care how many bad reviews purple rain ...     1      1\n",
              "3     This Norwegian film starts with a man jumping ...     1      1\n",
              "4     I have seen the short movie a few years ago. A...     1      1\n",
              "...                                                 ...   ...    ...\n",
              "7405  Why did the histories of Mary and Rhoda have t...     1      0\n",
              "7406  The movie is not that bad, Ringo Lam sucks. I ...     0      0\n",
              "7407  This picture started out with good intentions,...     0      0\n",
              "7408  One of the greatest lessons I ever had in how ...     1      0\n",
              "7409  It is the early morning of our discontent, and...     1      0\n",
              "\n",
              "[7410 rows x 3 columns]>"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4 - Model Evaluation"
      ],
      "metadata": {
        "id": "bh4CFUNkFGs0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, accuracy_score, f1_score, precision_score, recall_score\n",
        "from sklearn import metrics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8vRpF-Tw9tV_",
        "outputId": "ee645cc6-37d7-46e6-9609-504084b6e77f"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(binary_pred,test_targets))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wjOovnOh-W4X",
        "outputId": "b398ef10-5eeb-49dd-9d90-b3f180d42aac"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.57      0.93      0.70      2272\n",
            "           1       0.95      0.69      0.80      5138\n",
            "\n",
            "    accuracy                           0.76      7410\n",
            "   macro avg       0.76      0.81      0.75      7410\n",
            "weighted avg       0.84      0.76      0.77      7410\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the probabilities from the logits\n",
        "probabilities = []"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zY9bXgMywXI6",
        "outputId": "7423855b-4c3b-4e00-cbe5-90844b531b28"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for array in pred:\n",
        "\n",
        "    probabilities_tmp = np.exp(array) / np.sum(np.exp(array), axis=1, keepdims=True)\n",
        "    probabilities.append(probabilities_tmp)\n"
      ],
      "metadata": {
        "id": "LsNiMLcgu7XD"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_1_probabilities = [sub_array[:,1] for sub_array in probabilities]\n",
        "class_1_probabilities = np.concatenate(class_1_probabilities)"
      ],
      "metadata": {
        "id": "Bi7KW8eiwxWW"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "auc = metrics.roc_auc_score(test_targets, class_1_probabilities)"
      ],
      "metadata": {
        "id": "iuHTxBqI-gO2"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "auc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ySIWjwSI1-qf",
        "outputId": "2b03b093-bbd4-4238-c7d2-a5b00ef0a15c"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9018347384083587"
            ]
          },
          "metadata": {},
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   The dense neural network achieved an accuracy rate of 76.13% when classifying the test data. Among the reviews it predicted as having a positive sentiment, 95% were accurate in reality. In terms of recall, the model correctly identified 69% of the reviews that were genuinely positive.\n",
        "\n",
        "*   Examining the F1 scores, we find that the negative class has an F1 score of 0.7, while the positive class has an F1 score of 0.8. These scores signify a well-balanced performance, where the model adeptly balances the accurate identification of class 1 instances (recall) while simultaneously minimizing false positives (precision).\n",
        "\n",
        "\n",
        "*   The AUC (Area Under the Curve) is 90.18%, indicating that the model can effectively differentiate between positive and negative movie reviews approximately 90.18% of the time.\n",
        "\n",
        "*   The model displayed an accuracy rate of 77.22% on the training set and 76.13% on the testing set, suggesting that it did not exhibit overfitting, as the performance on both sets is relatively consistent.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2A0uIucUDRwi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5 - Testing with new movie reviews"
      ],
      "metadata": {
        "id": "3On3K5n1UP83"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(sentence):\n",
        "    inputs = np.array(convert_to_tensor(sentence, vocab_dict=Vocab))\n",
        "\n",
        "    # Batch size 1, add dimension for batch, to work with the model\n",
        "    inputs = inputs[None, :]\n",
        "\n",
        "    # predict with the model\n",
        "    preds_probs = model(inputs)\n",
        "    #print(preds_probs)\n",
        "\n",
        "    probabilities_tmp = np.exp(preds_probs) / np.sum(np.exp( preds_probs), axis=1, keepdims=True) ##Convert logits to probabilities\n",
        "\n",
        "    #print(probabilities_tmp)\n",
        "\n",
        "    # Turn probabilities into categories\n",
        "    preds = int(preds_probs[0, 1] > preds_probs[0, 0])\n",
        "\n",
        "    sentiment = \"negative\"\n",
        "    class_1_probability = probabilities_tmp[0, 1]\n",
        "    class_0_probability = probabilities_tmp[0, 0]\n",
        "\n",
        "    if preds == 1:\n",
        "        sentiment = 'positive'\n",
        "\n",
        "    return class_1_probability, class_0_probability, sentiment\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QVj7v9RUUgv8",
        "outputId": "c307c494-1993-4e16-a218-c00447e4ba92"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# try a positive review\n",
        "sentence = \"Even almost 15 years on this movie is still amazing to me, I remember my first time watching Meet The Robinsons i cried \\\n",
        "at the end because it was so heartwarming now every time I watch the movie i cry no matter how hard i try not to. \\\n",
        "The loving family aesthetic and the great story-line is truly wonderful a young boy searching for a mother who didn't \\\n",
        "want him even typing this up I'm tearing up as of now. If I had to watch one thing for my rest of my life i could happily watch \\\n",
        "this movie no questions asked it reminds me of how good life can be and how my life was good when i first watched it solid 10/10 !\""
      ],
      "metadata": {
        "id": "JeovPKppUjTy"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tmp_pred_1, tmp_pred_0, tmp_sentiment = predict(sentence)\n",
        "print(f\"The sentiment of the sentence is {tmp_sentiment}. (Positive Score: {tmp_pred_1}, Negative Score: {tmp_pred_0})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CTvxsILGU5Bm",
        "outputId": "3e616b23-5b8e-48a0-b38a-a91f5f8a902d"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The sentiment of the sentence is positive. (Positive Score: 0.9998065829277039, Negative Score: 0.0001933924067998305)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# try a negative sentence:\n",
        "sentence = \"Stiff acting and facial expressions.\\\n",
        "No conflict, she didn't have to fight for what she believes in that much, she was just accepted right away after they sent her off. \\\n",
        "No character development. She's already strong. She didn't even have to train properly. She just had to turn on that chi and that sucks. \\\n",
        "Unlikeable side characters.\\ They werent close enough so it did not justify that she was accepted by them \\\n",
        "Random guy with romance baiting. Her dynamics with the rest of the cast was weird and unnatural and forced. \\\n",
        "I liked her dad and the witch lady (kinda ish) though.The fighting was so whack.\\\n",
        "The attempts at comedy/funny times were so weird. Especially that part where she catches everything.\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5eSh9tt1UqdN",
        "outputId": "11956e74-e66c-478c-9bdf-4a9965e52163"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<>:2: DeprecationWarning: invalid escape sequence '\\ '\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tmp_pred_1, tmp_pred_0, tmp_sentiment = predict(sentence)\n",
        "print(f\"The sentiment of the sentence is {tmp_sentiment}. (Positive Score: {tmp_pred_1}, Negative Score: {tmp_pred_0})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i7HnfFDw8jK1",
        "outputId": "85deb181-a065-4870-ce3c-a0fe7243102e"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The sentiment of the sentence is negative. (Positive Score: 0.3185170292854309, Negative Score: 0.6814829707145691)\n"
          ]
        }
      ]
    }
  ]
}